{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEMcHnQuBUVM"
      },
      "outputs": [],
      "source": [
        "!rm -rf ~/.cache/huggingface/datasets/databricks__databricks-dolly-15k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0mqO5pKwBVYU",
        "outputId": "c05d6777-b7e7-41f6-a183-f52d1e8c2e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ca243587f8784ae9b2e44973cb5bf582",
            "7cc96a10e6324800898a56d15b259ada",
            "a46d7d7e18fa422ca9a76ad92523a443",
            "8a23407f775f4f09b21109cf35a7f971",
            "d9b8063761314881a71639b9bff8c237",
            "ee0a696555704ef59054127e8fc1e015",
            "35ef0231ec06464ba8a9697ab28eacf8",
            "3fa1c8248f5f44c79b8de7db29cf56ff",
            "df12946fa3f940518837d111923205e1",
            "e523bb940e3447169c332e617df56298",
            "d9934461ea9143a08ae532647a7f2978"
          ]
        },
        "id": "reG4Dh4as8_T",
        "outputId": "66b3ef0f-bc45-42eb-c705-5bfe5f4aec81"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca243587f8784ae9b2e44973cb5bf582",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7cc96a10e6324800898a56d15b259ada",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a46d7d7e18fa422ca9a76ad92523a443",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a23407f775f4f09b21109cf35a7f971",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9b8063761314881a71639b9bff8c237",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee0a696555704ef59054127e8fc1e015",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35ef0231ec06464ba8a9697ab28eacf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fa1c8248f5f44c79b8de7db29cf56ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df12946fa3f940518837d111923205e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e523bb940e3447169c332e617df56298",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9934461ea9143a08ae532647a7f2978",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/75 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Epoch 0: 100%|██████████| 75/75 [00:25<00:00,  2.95it/s, loss=14.8]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0] Loss: 14.8452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from typing import Tuple, Dict, Any, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# 기본 설정\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Tokenizer & 모델 로드\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# GPT‑2 는 pad_token 이 없으므로 eos_token 을 사용\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "teacher_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "teacher_model.eval()  # teacher 는 고정, gradient 계산 X\n",
        "student_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Soft Prompt 모듈\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "class SoftPrompt(nn.Module):\n",
        "    \"\"\"왼쪽에 붙는 learnable soft‑prompt (length, hidden)\"\"\"\n",
        "\n",
        "    def __init__(self, length: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.length = length\n",
        "        self.prompt = nn.Parameter(torch.randn(length, hidden_size))\n",
        "\n",
        "    def forward(self, input_embeds: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size = input_embeds.size(0)\n",
        "        prompt_expanded = self.prompt.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        return torch.cat([prompt_expanded, input_embeds], dim=1)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# 데이터셋 로드 & 전처리\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "raw_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
        "raw_dataset = raw_dataset.select(range(int(len(raw_dataset) * 0.02)))  # 2% 만 사용\n",
        "\n",
        "def preprocess(example: Dict[str, str]) -> Dict[str, str]:\n",
        "    x = f\"{example['instruction']}\\n{example['context']}\"\n",
        "    y = example[\"response\"]\n",
        "    return {\"x\": x, \"y\": y}\n",
        "\n",
        "dataset = raw_dataset.map(preprocess)\n",
        "\n",
        "# Collate Fn --------------------------------------------------------------------\n",
        "\n",
        "def collate_fn(batch: Any) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:\n",
        "    inputs = tokenizer([item[\"x\"] for item in batch],\n",
        "                       return_tensors=\"pt\",\n",
        "                       padding=True,\n",
        "                       truncation=True)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer([item[\"y\"] for item in batch],\n",
        "                           return_tensors=\"pt\",\n",
        "                           padding=True,\n",
        "                           truncation=True).input_ids\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    labels = labels.to(device)\n",
        "    return inputs, labels\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# 학습 파라미터 & 옵티마이저\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "soft_prompt_len = 7\n",
        "hidden_size = student_model.config.hidden_size\n",
        "soft_prompt = SoftPrompt(length=soft_prompt_len, hidden_size=hidden_size).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    list(student_model.parameters()) + list(soft_prompt.parameters()),\n",
        "    lr=5e-5,\n",
        ")\n",
        "\n",
        "max_pos = student_model.config.n_positions        # GPT‑2 기본 1024\n",
        "trim_len = max_pos - soft_prompt_len              # 실제 본문이 가질 수 있는 최대 길이\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# 학습 루프\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "loader = DataLoader(dataset,\n",
        "                    batch_size=4,\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(1):\n",
        "    progress = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for inputs, _ in progress:\n",
        "        # ── 시퀀스 길이 잘라내기 ────────────────────────────────────────────\n",
        "        input_ids: torch.Tensor = inputs[\"input_ids\"]\n",
        "        attention_mask: torch.Tensor = inputs[\"attention_mask\"]\n",
        "\n",
        "        if input_ids.size(1) > trim_len:\n",
        "            input_ids = input_ids[:, :trim_len]\n",
        "            attention_mask = attention_mask[:, :trim_len]\n",
        "\n",
        "        # teacher 입력도 동일하게 잘라낸 것으로 교체\n",
        "        trimmed_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "        # ── Teacher Forward (No‑grad) ────────────────────────────────────────\n",
        "        with torch.no_grad():\n",
        "            teacher_outputs = teacher_model(**trimmed_inputs)\n",
        "            teacher_probs = F.softmax(teacher_outputs.logits, dim=-1)\n",
        "\n",
        "        # ── Student Forward ─────────────────────────────────────────────────\n",
        "        input_embeds = student_model.transformer.wte(input_ids)\n",
        "        input_embeds = soft_prompt(input_embeds)  # (B, prompt+seq, H)\n",
        "\n",
        "        # attention mask: prompt 영역 1 로 padding\n",
        "        prompt_mask = torch.ones(input_ids.size(0), soft_prompt_len,\n",
        "                                 dtype=attention_mask.dtype,\n",
        "                                 device=device)\n",
        "        extended_attention_mask = torch.cat([prompt_mask, attention_mask], dim=1)\n",
        "\n",
        "        # position ids: 0 ~ L‑1\n",
        "        seq_len = input_embeds.size(1)\n",
        "        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
        "\n",
        "        student_outputs = student_model(inputs_embeds=input_embeds,\n",
        "                                        attention_mask=extended_attention_mask,\n",
        "                                        position_ids=position_ids)\n",
        "        student_log_probs = F.log_softmax(student_outputs.logits, dim=-1)\n",
        "\n",
        "        # ── KL Loss (student_probs exclude prompt tokens) ────────────────────\n",
        "        loss = F.kl_div(student_log_probs[:, soft_prompt_len:, :],\n",
        "                        teacher_probs,\n",
        "                        reduction=\"batchmean\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12So_xe0Ca2N"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioq6UiSDC_LR"
      },
      "source": [
        "## Loss Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr_JCdZVCbbs"
      },
      "outputs": [],
      "source": [
        "#entropy = -(teacher_probs * teacher_probs.log()).sum(dim=-1)\n",
        "#weight = 1 - entropy / math.log(vocab_size)\n",
        "#kl = F.kl_div(...).sum(dim=-1)\n",
        "#weighted_kl = (weight * kl).mean()\n",
        "def weighted_loss(kl_loss: torch.Tensor, ce_loss: Optional[torch.Tensor] = None, *, alpha: float = 1.0, beta: float = 0.0) -> torch.Tensor:\n",
        "    \"\"\"KL + CrossEntropy 가중합. beta=0 → pure KL\n",
        "    Args:\n",
        "        kl_loss : scalar\n",
        "        ce_loss : scalar or None\n",
        "        alpha   : weight for KL\n",
        "        beta    : weight for CE\n",
        "    \"\"\"\n",
        "    if ce_loss is None:\n",
        "        return kl_loss * alpha\n",
        "    return alpha * kl_loss + beta * ce_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkR5lea4DG0k"
      },
      "source": [
        "## Soft Blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Abt_9WoDRqD"
      },
      "outputs": [],
      "source": [
        "#blend = alpha * teacher_probs + (1 - alpha) * student_probs\n",
        "#F.kl_div(student_log_probs, blend, reduction='batchmean')\n",
        "def soft_blend(teacher_probs: torch.Tensor, student_log_probs: torch.Tensor, gamma: float = 0.5) -> torch.Tensor:\n",
        "    \"\"\"Teacher 와 Student 예측을 γ 비율로 혼합해 새로운 타깃 확률 생성\n",
        "    Args:\n",
        "        teacher_probs: (B, L, V) softmax\n",
        "        student_log_probs: (B, L, V) log‑softmax\n",
        "    Returns:\n",
        "        blended_probs: (B, L, V)\n",
        "    \"\"\"\n",
        "    student_probs = student_log_probs.exp().detach()  # stop grad\n",
        "    blended = gamma * teacher_probs + (1.0 - gamma) * student_probs\n",
        "    return blended / blended.sum(dim=-1, keepdim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1epBN229pIfk"
      },
      "source": [
        "##Teacher Assistant Filterting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFUa_L1vpHbc"
      },
      "outputs": [],
      "source": [
        "def teacher_assistant_filter(probs: torch.Tensor, *, top_k: Optional[int] = None, top_p: Optional[float] = None) -> torch.Tensor:\n",
        "    \"\"\"Top‑k 또는 Top‑p nucleus 필터링 후 확률 재정규화\n",
        "    Args:\n",
        "        probs: (B, L, V)\n",
        "    Returns:\n",
        "        filtered_probs: (B, L, V)   (gradient ✗)  – no grad assumed (teacher output)\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        if top_k is not None:\n",
        "            top_k = max(top_k, 1)\n",
        "            vals, idx = torch.topk(probs, top_k)\n",
        "            mask = torch.zeros_like(probs).scatter_(2, idx, 1.0)\n",
        "            probs = probs * mask\n",
        "        if top_p is not None:\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "            cumulative = sorted_probs.cumsum(dim=-1)\n",
        "            mask = (cumulative - sorted_probs) < top_p  # first token that makes cum>p 포함\n",
        "            new_mask = torch.zeros_like(probs).scatter_(2, sorted_idx, mask)\n",
        "            probs = probs * new_mask\n",
        "        # re‑normalize\n",
        "        probs = probs / probs.sum(dim=-1, keepdim=True).clamp(min=1e-9)\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJDdbaiyvGO",
        "outputId": "bf140260-8dd5-4871-e884-95d4b2c7eacf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 75/75 [00:25<00:00,  2.89it/s, loss=53.5835]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0] Loss: 53.5835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습 파라미터\n",
        "\n",
        "soft_len = 7\n",
        "hidden = student_model.config.hidden_size\n",
        "soft_prompt = SoftPrompt(soft_len, hidden).to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(list(student_model.parameters()) + list(soft_prompt.parameters()), lr=5e-5)\n",
        "\n",
        "max_pos = student_model.config.n_positions  # 1024\n",
        "trim_len = max_pos - soft_len\n",
        "\n",
        "top_k_assist = 50       # teacher filtering\n",
        "blend_gamma = 0.7        # soft blend ratio (teacher dominant)\n",
        "alpha_kl = 1.0\n",
        "beta_ce = 0.0            # if >0, CE(y)도 사용\n",
        "\n",
        "# 학습 루프(각 메소드별로 나눠서 아래와 같은 루프 3개 돌려보기)\n",
        "\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "for epoch in range(1):\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n",
        "    for inputs, labels in pbar:\n",
        "        # ── trim ────────────────────────────────────────────────────────────\n",
        "        input_ids, attention_mask = inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
        "        if input_ids.size(1) > trim_len:\n",
        "            input_ids = input_ids[:, :trim_len]\n",
        "            attention_mask = attention_mask[:, :trim_len]\n",
        "        trimmed_inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
        "\n",
        "        # ── Teacher ─────────────────────────────────────────────────────────\n",
        "        with torch.no_grad():\n",
        "            t_logits = teacher_model(**trimmed_inputs).logits\n",
        "            t_probs = F.softmax(t_logits, dim=-1)\n",
        "            t_probs = teacher_assistant_filter(t_probs, top_k=top_k_assist)\n",
        "\n",
        "        # ── Student forward ────────────────────────────────────────────────\n",
        "        embeds = student_model.transformer.wte(input_ids)\n",
        "        embeds = soft_prompt(embeds)\n",
        "\n",
        "        prompt_mask = torch.ones(input_ids.size(0), soft_len, dtype=attention_mask.dtype, device=device)\n",
        "        ext_mask = torch.cat([prompt_mask, attention_mask], dim=1)\n",
        "        seq_len = embeds.size(1)\n",
        "        pos_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(input_ids.size(0), -1)\n",
        "\n",
        "        s_logits = student_model(inputs_embeds=embeds, attention_mask=ext_mask, position_ids=pos_ids).logits\n",
        "        s_log_probs = F.log_softmax(s_logits, dim=-1)\n",
        "\n",
        "        # ── Soft Blending ─────────────────────────────────────────────────\n",
        "        target_probs = soft_blend(t_probs, s_log_probs[:, soft_len:, :], gamma=blend_gamma)\n",
        "\n",
        "        # ── Loss 계산 ──────────────────────────────────────────────────────\n",
        "        kl_loss = F.kl_div(s_log_probs[:, soft_len:, :], target_probs, reduction=\"batchmean\")\n",
        "        # Cross‑entropy(teacher vs GT)가 필요하면 labels 사용\n",
        "        ce_loss = None\n",
        "        if beta_ce > 0:\n",
        "            ce_loss = F.cross_entropy(t_logits.view(-1, t_logits.size(-1)), labels.view(-1), ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "        loss = weighted_loss(kl_loss, ce_loss, alpha=alpha_kl, beta=beta_ce)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
        "\n",
        "    print(f\"[Epoch {epoch}] Loss: {loss.item():.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}